{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    RegexpTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    "    WordPunctTokenizer,\n",
    ")\n",
    "from nltk.corpus import stopwords, wordnet, movie_reviews\n",
    "import random, nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.0 tokenize]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.1 sent_tokenize & word_tokenize]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting Text by sentence: \n",
      "\n",
      "Hello Mr. Hany, how are you doing today?\n",
      "The weater is great and Python is awesome.\n",
      "The sky is pinhish-blue.\n",
      "You should not eat carboard. \n",
      "\n",
      "Spliting Text by Words: \n",
      "\n",
      "Hello - Mr. - Hany - , - how - are - you - doing - today - ? - The - weater - is - great - and - Python - is - awesome. - The - sky - is - pinhish-blue. - You - should - not - eat - carboard - .\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hello Mr. Hany, how are you doing today? The weater is great and Python is awesome.\n",
    "        The sky is pinhish-blue. You should not eat carboard.\"\"\"\n",
    "\n",
    "# sent_tokenize: split the text by sentence\n",
    "\n",
    "\n",
    "print(\"Spliting Text by sentence: \\n\")\n",
    "print(\"\\n\".join(sent_tokenize(text=text, language=\"english\")), \"\\n\")\n",
    "\n",
    "# word_tokenize: split the text by Words\n",
    "print(\"Spliting Text by Words: \\n\")\n",
    "print(\" - \".join(word_tokenize(text=text, language=\"english\", preserve_line=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.2 RegexpTokenizer]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n"
     ]
    }
   ],
   "source": [
    "text = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "# RegexpTokenizer: used like regular expressions, which takes a letter to split based on them\n",
    "\n",
    "took = RegexpTokenizer(r\"\\w+|\\$[\\d\\.]+|\\S+[.]+\")\n",
    "\n",
    "print(took.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.3 WhitespaceTokenizer and WordPunctTokenizer]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer: Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+.\n",
    "took = WordPunctTokenizer()\n",
    "\n",
    "print(took.tokenize(text))\n",
    "\n",
    "\n",
    "# WhitespaceTokenizer : Tokenize a string on whitespace (space, tab, newline). In general, users should use the string split() method instead.\n",
    "took = WhitespaceTokenizer()\n",
    "\n",
    "print(took.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2.0 Stop]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.1 stop words]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'showing', 'stop', 'words', 'filtration', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(fileids=\"english\"))\n",
    "# Stop words are a set of commonly used words in a language, Stop words are commonly used NLP, in which they are removed from the sentance\n",
    "# filter the words based on the stop words in the language\n",
    "text = \"This is an example showing off stop words filtration.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering :  ['This', 'is', 'an', 'example', 'showing', 'off', 'stop', 'words', 'filtration', '.'] \n",
      "\n",
      "after filtering :  ['This', 'example', 'showing', 'stop', 'words', 'filtration', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(fileids=\"english\"))\n",
    "# Stop words are a set of commonly used words in a language, Stop words are commonly used NLP, in which they are removed from the sentance\n",
    "# filter the words based on the stop words in the language\n",
    "text = \"This is an example showing off stop words filtration.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "print(\"Before filtering : \", words, \"\\n\")\n",
    "print(\"after filtering : \", filtered_words, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3.0 stem]()\n",
    "\n",
    "Interfaces used to remove morphological affixes from words, leaving only the word stem. Stemming algorithms aim to remove those affixes required for eg. grammatical role, tense, derivational morphology leaving only the stem of the word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3.1 porter]()\n",
    "\n",
    "An algorithm for suffix stripping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n",
      "owe\n",
      "say\n",
      "meant\n",
      "cement\n",
      "\n",
      "words before :  ['It', 'is', 'very', 'imporatant', 'to', 'be', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'ones', '.'] \n",
      "\n",
      "words after :  {'is', 'veri', 'at', 'have', 'it', 'least', 'to', 'python', 'while', '.', 'pythonli', 'one', 'you', 'be', 'with', 'poorli', 'all', 'are', 'imporat'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "some_words = [\n",
    "    \"maximum\",\n",
    "    \"presumably\",\n",
    "    \"multiply\",\n",
    "    \"provision\",\n",
    "    \"owed\",\n",
    "    \"saying\",\n",
    "    \"meant\",\n",
    "    \"cement\",\n",
    "]\n",
    "\n",
    "\n",
    "for w in some_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "print()\n",
    "\n",
    "text = \"\"\"\n",
    "    It is very imporatant to be pythonly while you are pythoning with python.\n",
    "    All pythoners have pythoned poorly at least ones.\n",
    "\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"words before : \", words, \"\\n\")\n",
    "\n",
    "\n",
    "filtered_words = set()\n",
    "\n",
    "for w in words:\n",
    "    filtered_words.add(ps.stem(w))\n",
    "\n",
    "print(\"words after : \", filtered_words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3.2 LancasterStemmer]()\n",
    "\n",
    "The Lancaster Stemmer is an algorithm for stemming words in the English language, It aims to efficiently remove prefixes and suffixes from words to find their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim\n",
      "presum\n",
      "multiply\n",
      "provid\n",
      "ow\n",
      "say\n",
      "meant\n",
      "cem\n",
      "met\n",
      "\n",
      "words before :  ['It', 'is', 'very', 'imporatant', 'to', 'be', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'ones', '.'] \n",
      "\n",
      "words after :  {'very', 'whil', 'poor', 'is', 'to', 'imp', 'python', 'you', 'ar', 'on', '.', 'hav', 'at', 'be', 'it', 'al', 'least', 'with'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps = LancasterStemmer(strip_prefix_flag=True)\n",
    "\n",
    "some_words = [\n",
    "    \"maximum\",\n",
    "    \"presumably\",\n",
    "    \"multiply\",\n",
    "    \"provision\",\n",
    "    \"owed\",\n",
    "    \"saying\",\n",
    "    \"meant\",\n",
    "    \"cement\",\n",
    "    \"kilometer\",\n",
    "]\n",
    "for w in some_words:\n",
    "    print(ps.stem(w))\n",
    "print()\n",
    "\n",
    "text = \"\"\"It is very imporatant to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least ones.\"\"\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"words before : \", words, \"\\n\")\n",
    "\n",
    "filtered_words = set()\n",
    "\n",
    "for w in words:\n",
    "    filtered_words.add(ps.stem(w))\n",
    "\n",
    "print(\"words after : \", filtered_words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3.3 WordNetLemmatizer]()\n",
    "\n",
    "It's used for lemmatization, which is the process of reducing words to their base or root form, known as the lemma.\n",
    "\n",
    "For example:\n",
    "\n",
    "    The lemma of \"running\" is \"run\".\n",
    "\n",
    "    The lemma of \"better\" is \"good\".\n",
    "\n",
    "Lemmatization is useful in NLP tasks for standardizing words so that variations of the same word are treated as identical, which can improve the accuracy of analyses like text classification or information retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "dog\n",
      "church\n",
      "hardrock\n",
      "read\n",
      "reader\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(word=\"running\", pos=\"v\"))\n",
    "print(lemma.lemmatize(\"dogs\"))\n",
    "print(lemma.lemmatize(\"churches\"))\n",
    "print(lemma.lemmatize(\"hardrock\", pos=\"v\"))\n",
    "print(lemma.lemmatize(\"reading\", pos=\"v\"))\n",
    "print(lemma.lemmatize(\"reader\", pos=\"v\"))\n",
    "print(lemma.lemmatize(\"better\"))\n",
    "print(lemma.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.0 wordnet]()\n",
    "\n",
    "WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, each expressing a distinct concept. These synsets are interconnected by means of conceptual-semantic and lexical relations\n",
    "\n",
    "Here are some key features of WordNet:\n",
    "\n",
    "- Synonym Sets (Synsets): WordNet organizes words into synsets, which are groups of words that are synonymous with each other. For example, the synset for the word \"car\" might include synonyms such as \"automobile\" and \"vehicle\".\n",
    "\n",
    "- Antonyms: WordNet includes antonyms for many words, allowing for the representation of opposite concepts. For example, \"hot\" is an antonym of \"cold\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets of word good : \n",
      "\n",
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "all words of word good : \n",
      "\n",
      "{'commodity', 'adept', 'good', 'thoroughly', 'well', 'full', 'estimable', 'effective', 'dear', 'dependable', 'beneficial'}\n"
     ]
    }
   ],
   "source": [
    "snys = wordnet.synsets(\"good\")\n",
    "\n",
    "# synsets\n",
    "print(\"synsets of word good : \\n\")\n",
    "print(snys)\n",
    "\n",
    "# words\n",
    "\n",
    "words = set()\n",
    "\n",
    "for i, s in enumerate(snys):\n",
    "    words.add(snys[i].lemmas()[0].name())\n",
    "\n",
    "print(\"all words of word good : \\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words of word good : \n",
      "\n",
      "{'unspoilt', 'sound', 'well', 'practiced', 'honest', 'beneficial', 'commodity', 'right', 'in_force', 'respectable', 'goodness', 'soundly', 'effective', 'near', 'dear', 'proficient', 'upright', 'in_effect', 'adept', 'skillful', 'ripe', 'thoroughly', 'full', 'estimable', 'skilful', 'salutary', 'dependable', 'expert', 'trade_good', 'just', 'undecomposed', 'safe', 'good', 'unspoiled', 'honorable', 'serious', 'secure'} \n",
      "\n",
      "words and thier antonyms :\n",
      "\n",
      "[['good', 'evil'], ['goodness', 'evilness'], ['good', 'bad'], ['goodness', 'badness'], ['good', 'bad'], ['good', 'evil'], ['well', 'ill']]\n"
     ]
    }
   ],
   "source": [
    "synonyms = set()\n",
    "antonyms = []\n",
    "for syn in snys:\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.add(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append([l.name(), l.antonyms()[0].name()])\n",
    "\n",
    "\n",
    "print(\"all words of word good : \\n\")\n",
    "print(synonyms, \"\\n\")\n",
    "print(\"words and thier antonyms :\\n\")\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.0 Wu Palmer Similarity]()\n",
    "\n",
    "used to get the Similarity percentage between two words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cake.n.01'), Synset('patty.n.01'), Synset('cake.n.03'), Synset('coat.v.03')] \n",
      "\n",
      "[Synset('loaf_of_bread.n.01'), Synset('loaf.n.02'), Synset('bum.v.02'), Synset('loiter.v.01')] \n",
      "\n",
      "[Synset('bread.n.01'), Synset('boodle.n.01'), Synset('bread.v.01')] \n",
      "\n",
      "cake \n",
      "\n",
      "Synset('loaf_of_bread.n.01') \n",
      "\n",
      "Synset('bread.n.01') \n",
      "\n",
      "cake  is Similar to  loaf_of_bread  by  26.666666666666668 \n",
      "\n",
      "cake  is Similar to  bread  by  28.57142857142857 \n",
      "\n",
      "loaf_of_bread  is Similar to  cake  by  26.666666666666668 \n",
      "\n",
      "loaf_of_bread  is Similar to  bread  by  94.11764705882352 \n",
      "\n",
      "bread  is Similar to  cake  by  28.57142857142857 \n",
      "\n",
      "bread  is Similar to  loaf_of_bread  by  94.11764705882352 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synsets(\"cake\")\n",
    "w2 = wordnet.synsets(\"loaf\")\n",
    "w3 = wordnet.synsets(\"bread\")\n",
    "cake = w1[0]\n",
    "loaf = w2[0]\n",
    "bread = w3[0]\n",
    "print(w1, \"\\n\")\n",
    "print(w2, \"\\n\")\n",
    "print(w3, \"\\n\")\n",
    "print(cake.lemmas()[0].name(), \"\\n\")\n",
    "print(loaf, \"\\n\")\n",
    "print(bread, \"\\n\")\n",
    "\n",
    "words = [cake, loaf, bread]\n",
    "\n",
    "for w in words:\n",
    "    for ww in words:\n",
    "        if w != ww:\n",
    "            print(\n",
    "                w.lemmas()[0].name(),\n",
    "                \" is Similar to \",\n",
    "                ww.lemmas()[0].name(),\n",
    "                \" by \",\n",
    "                w.wup_similarity(ww) * 100,\n",
    "                \"\\n\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cake  is Similar to  loaf_of_bread  by  26.666666666666668 \n",
      "\n",
      "cake  is Similar to  bread  by  28.57142857142857 \n",
      "\n",
      "loaf_of_bread  is Similar to  cake  by  26.666666666666668 \n",
      "\n",
      "loaf_of_bread  is Similar to  bread  by  94.11764705882352 \n",
      "\n",
      "bread  is Similar to  cake  by  28.57142857142857 \n",
      "\n",
      "bread  is Similar to  loaf_of_bread  by  94.11764705882352 \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5.0 Text Classification problem]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc =  91.3\n",
      "test acc =  94.0\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileid)), cat)\n",
    "    for cat in movie_reviews.categories()\n",
    "    for fileid in movie_reviews.fileids(cat)\n",
    "]\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:6000]\n",
    "\n",
    "\n",
    "def find_features(document):\n",
    "\n",
    "    words = set(document)\n",
    "\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = w in words\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "features = [(find_features(review), cat) for (review, cat) in documents]\n",
    "\n",
    "\n",
    "train = features[:2000]\n",
    "test = features[1900:]\n",
    "\n",
    "\n",
    "clf = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "print(\"train acc = \", nltk.classify.accuracy(clf, train) * 100)\n",
    "print(\"test acc = \", nltk.classify.accuracy(clf, test) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
