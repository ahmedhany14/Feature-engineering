{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    sent_tokenize,\n",
    "    word_tokenize,\n",
    "    RegexpTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    "    WordPunctTokenizer,\n",
    ")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1.0 tokenize]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.1 sent_tokenize & word_tokenize]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting Text by sentence: \n",
      "\n",
      "Hello Mr. Hany, how are you doing today?\n",
      "The weater is great and Python is awesome.\n",
      "The sky is pinhish-blue.\n",
      "You should not eat carboard. \n",
      "\n",
      "Spliting Text by Words: \n",
      "\n",
      "Hello - Mr. - Hany - , - how - are - you - doing - today - ? - The - weater - is - great - and - Python - is - awesome. - The - sky - is - pinhish-blue. - You - should - not - eat - carboard - .\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hello Mr. Hany, how are you doing today? The weater is great and Python is awesome.\n",
    "        The sky is pinhish-blue. You should not eat carboard.\"\"\"\n",
    "\n",
    "# sent_tokenize: split the text by sentence\n",
    "\n",
    "\n",
    "print(\"Spliting Text by sentence: \\n\")\n",
    "print(\"\\n\".join(sent_tokenize(text=text, language=\"english\")), \"\\n\")\n",
    "\n",
    "# word_tokenize: split the text by Words\n",
    "print(\"Spliting Text by Words: \\n\")\n",
    "print(\" - \".join(word_tokenize(text=text, language=\"english\", preserve_line=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.2 RegexpTokenizer]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n"
     ]
    }
   ],
   "source": [
    "text = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "# RegexpTokenizer: used like regular expressions, which takes a letter to split based on them\n",
    "\n",
    "took = RegexpTokenizer(r\"\\w+|\\$[\\d\\.]+|\\S+[.]+\")\n",
    "\n",
    "print(took.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.3 WhitespaceTokenizer and WordPunctTokenizer]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer: Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+.\n",
    "took = WordPunctTokenizer()\n",
    "\n",
    "print(took.tokenize(text))\n",
    "\n",
    "\n",
    "# WhitespaceTokenizer : Tokenize a string on whitespace (space, tab, newline). In general, users should use the string split() method instead.\n",
    "took = WhitespaceTokenizer()\n",
    "\n",
    "print(took.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2.0 Stop]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2.1 stop words]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'showing', 'stop', 'words', 'filtration', '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(fileids=\"english\"))\n",
    "# Stop words are a set of commonly used words in a language, Stop words are commonly used NLP, in which they are removed from the sentance\n",
    "# filter the words based on the stop words in the language\n",
    "text = \"This is an example showing off stop words filtration.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering :  ['This', 'is', 'an', 'example', 'showing', 'off', 'stop', 'words', 'filtration', '.'] \n",
      "\n",
      "after filtering :  ['This', 'example', 'showing', 'stop', 'words', 'filtration', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(fileids=\"english\"))\n",
    "# Stop words are a set of commonly used words in a language, Stop words are commonly used NLP, in which they are removed from the sentance\n",
    "# filter the words based on the stop words in the language\n",
    "text = \"This is an example showing off stop words filtration.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "print(\"Before filtering : \", words, \"\\n\")\n",
    "print(\"after filtering : \", filtered_words, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3.0 stem]()\n",
    "\n",
    "Interfaces used to remove morphological affixes from words, leaving only the word stem. Stemming algorithms aim to remove those affixes required for eg. grammatical role, tense, derivational morphology leaving only the stem of the word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3.1 porter]()\n",
    "\n",
    "An algorithm for suffix stripping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n",
      "owe\n",
      "say\n",
      "meant\n",
      "cement\n",
      "\n",
      "words before :  ['It', 'is', 'very', 'imporatant', 'to', 'be', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'ones', '.'] \n",
      "\n",
      "words after :  {'.', 'to', 'pythonli', 'veri', 'poorli', 'it', 'least', 'with', 'imporat', 'are', 'is', 'one', 'be', 'python', 'while', 'have', 'all', 'at', 'you'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "some_words = [\n",
    "    \"maximum\",\n",
    "    \"presumably\",\n",
    "    \"multiply\",\n",
    "    \"provision\",\n",
    "    \"owed\",\n",
    "    \"saying\",\n",
    "    \"meant\",\n",
    "    \"cement\",\n",
    "]\n",
    "\n",
    "\n",
    "for w in some_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "print()\n",
    "\n",
    "text = \"\"\"\n",
    "    It is very imporatant to be pythonly while you are pythoning with python.\n",
    "    All pythoners have pythoned poorly at least ones.\n",
    "\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"words before : \", words, \"\\n\")\n",
    "\n",
    "\n",
    "filtered_words = set()\n",
    "\n",
    "for w in words:\n",
    "    filtered_words.add(ps.stem(w))\n",
    "\n",
    "print(\"words after : \", filtered_words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3.2 LancasterStemmer]()\n",
    "\n",
    "The Lancaster Stemmer is an algorithm for stemming words in the English language, It aims to efficiently remove prefixes and suffixes from words to find their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxim\n",
      "presum\n",
      "multiply\n",
      "provid\n",
      "ow\n",
      "say\n",
      "meant\n",
      "cem\n",
      "met\n",
      "\n",
      "words before :  ['It', 'is', 'very', 'imporatant', 'to', 'be', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'ones', '.'] \n",
      "\n",
      "words after :  {'hav', '.', 'at', 'you', 'al', 'to', 'poor', 'least', 'python', 'very', 'ar', 'on', 'be', 'whil', 'imp', 'with', 'is', 'it'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps = LancasterStemmer(strip_prefix_flag=True)\n",
    "\n",
    "some_words = [\n",
    "    \"maximum\",\n",
    "    \"presumably\",\n",
    "    \"multiply\",\n",
    "    \"provision\",\n",
    "    \"owed\",\n",
    "    \"saying\",\n",
    "    \"meant\",\n",
    "    \"cement\",\n",
    "    \"kilometer\",\n",
    "]\n",
    "for w in some_words:\n",
    "    print(ps.stem(w))\n",
    "print()\n",
    "\n",
    "text = \"\"\"It is very imporatant to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least ones.\"\"\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"words before : \", words, \"\\n\")\n",
    "\n",
    "filtered_words = set()\n",
    "\n",
    "for w in words:\n",
    "    filtered_words.add(ps.stem(w))\n",
    "\n",
    "print(\"words after : \", filtered_words, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
